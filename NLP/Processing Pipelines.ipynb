{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d282a-7735-422f-bfae-08dc406e4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All pipeline packages you can load into spaCy include several files and a config.cfg\n",
    "# Built-in components need binary data to make predictions\n",
    "# Pipeline attributes:\n",
    "    # nlp.pipe_names - list pipeline component names\n",
    "    # nlp.pipeline - list of (name, component) tuples\n",
    "# The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f06811-33d9-49f1-b7ea-71bca9cf5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000029B89CCF830>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000029B89B9ABD0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000029B89BCA570>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000029B89E96A10>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000029B89E79C10>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000029B89BCA1F0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d6517-0757-48ec-b496-f8255d159035",
   "metadata": {},
   "source": [
    "# Spacy Custom Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d7d13b-a592-4853-91dd-79d38923522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function execute automatically when you call nlp\n",
    "# Add your own metadata to documents and tokens\n",
    "# Updating built-in attributes like doc.ents\n",
    "\n",
    "# Custom component anatomy:\n",
    "# Function that takes a doc, modifies it and returns it\n",
    "# Registered using the Language.component decorator\n",
    "# Can be added using the nlp.add_pipe method, takes two parameters, \n",
    "    # \"component name\"\n",
    "    # position in the pipeline last | first boolean OR before | after \"name of existing component\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1d079f-cd83-4491-b1aa-f98d5bdd25e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955b5784-51ab-417b-8d30-7b8d6b3acaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(f'animal patterns: {animal_patterns}')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed7a17-358b-41ca-9c2e-6f1dbf382807",
   "metadata": {},
   "source": [
    "# Custom Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08502bfb-5fdd-4fe0-96d7-a6265d466cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the Doc, Token and Span objects to store custom data\n",
    "# doc._.title = \"My document\"\n",
    "# token._.is_color = True\n",
    "# span._.has_color = False\n",
    "\n",
    "# Registered on the global Doc, Token or Span using the set_extension method\n",
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "# Set extensions on the Doc, Token and Span\n",
    "# Doc.set_extension(\"title\", default=None)\n",
    "# Token.set_extension(\"is_color\", default=False)\n",
    "# Span.set_extension(\"has_color\", default=False)\n",
    "\n",
    "# There are three types of extensions: attribute extensions, property extensions and method extensions.\n",
    "# 1. attribute extensions\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "Overwrite extension attribute value\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "# 2. property extensions\n",
    "# Define a getter and an optional setter function\n",
    "# Getter only called when you retrieve the attribute value\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)\n",
    "\n",
    "# Span extensions should almost always use a getter\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)\n",
    "\n",
    "# 3. Method extensions\n",
    "# Assign a function that becomes available as an object method\n",
    "# Lets you pass arguments to the extension function\n",
    "# Define method with arguments, first arg is always the object itself\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8d8cc-c6fa-49a9-8cd8-78ffc3339d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631878f-1592-43e2-adf7-6b6f01881fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b5e42-90b9-4256-bcdc-0aa2bf303b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8631a7e-9659-4c72-8d02-6dcefcc9f34f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'wikipedia_url' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/w/index.php?search=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m entity_text\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set the Span extension wikipedia_url using the getter get_wikipedia_url\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m Span\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, getter\u001b[38;5;241m=\u001b[39mget_wikipedia_url)\n\u001b[0;32m     17\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn over fifty years from his very first recordings right through to his \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast album, David Bowie was at the vanguard of contemporary culture.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Print the text and Wikipedia URL of the entity\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\tokens\\span.pyx:44\u001b[0m, in \u001b[0;36mspacy.tokens.span.Span.set_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E090] Extension 'wikipedia_url' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02bd2c-3ad9-4db1-a7c1-0de81941fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "# Reads a JSON file mapping country names to their capitals (a dictionary).\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "# Creates a blank spaCy NLP pipeline for English, which starts with no components.\n",
    "nlp = spacy.blank(\"en\")\n",
    "# PhraseMatcher is created to detect country names in text.\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "# Converts each country name into a Doc object for efficient matching.\n",
    "# associates the name \"COUNTRY\" with these patterns\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "# custom pipeline component \n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "  \t# Use the matcher to find country names in a document.\n",
    "    matches = matcher(doc)\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    # Add these spans to doc.ents (the list of recognized entities in the document)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# getter is defined for the capital attribute using a lambda function\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "# Whenever the capital attribute is accessed on a Span, it looks up the span's text in \n",
    "# the CAPITALS dictionary and returns the corresponding value\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072b900-e655-4176-89c9-65be2e747b02",
   "metadata": {},
   "source": [
    "# Scaling & Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc328d-c291-401f-ab22-ce2e674a1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Process large volumes of text\n",
    "# Processes texts as a stream, yields Doc objects\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "# Setting as_tuples=True on nlp.pipe lets you pass in (text, context) tuples\n",
    "# Yields (doc, context) tuples\n",
    "# Useful for associating metadata with the doc\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])\n",
    "\n",
    "# add metadata as custom attribute \n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]\n",
    "\n",
    "# 2. Use only the tokenizer\n",
    "# Use nlp.make_doc to turn a text into a Doc object\n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "# Disable tagger and parser\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "# Restores them after the with block\n",
    "# Only runs the remaining components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cad1a3-0344-458a-8c21-7b5de62d08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e98372-17b6-4476-874b-172a17f52749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91005a-8260-4598-b094-1bc82ae853b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom attributes to add author and book meta information to quotes.\n",
    "# List of [text, context] examples is available as the variable DATA. \n",
    "# The texts are quotes from famous books, and the contexts dictionaries with the keys \"author\" and \"book\".\n",
    "# Use the set_extension method to register the custom attributes \"author\" and \"book\" on the Doc, which default to None.\n",
    "# Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "# Overwrite the doc._.book and doc._.author with the respective info passed in as the context.\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/en/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuple=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aca314c-6663-4f72-a142-fe5837f71dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdacac33-9846-4299-b16c-8880a2d8ecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and lemmatizer\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"lemmatizer\"]):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8df5c-93f8-4f48-92cb-aeb008e328aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
