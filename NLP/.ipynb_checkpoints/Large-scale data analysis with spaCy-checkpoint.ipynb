{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da017813-b0fb-4da3-81e2-3e66f3977ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 2.6/33.5 MB 15.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 7.3/33.5 MB 18.9 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 11.5/33.5 MB 19.0 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 18.1/33.5 MB 22.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 22.8/33.5 MB 21.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 29.6/33.5 MB 23.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  33.3/33.5 MB 24.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 33.5/33.5 MB 22.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae699a6-8e7b-47e1-9f57-f00c8bb250b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\thinc\\api.py:23\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     configure_normal_init,\n\u001b[0;32m     18\u001b[0m     glorot_uniform_init,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     zero_init,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     LSTM,\n\u001b[0;32m     25\u001b[0m     CauchySimilarity,\n\u001b[0;32m     26\u001b[0m     ClippedLinear,\n\u001b[0;32m     27\u001b[0m     Dish,\n\u001b[0;32m     28\u001b[0m     Dropout,\n\u001b[0;32m     29\u001b[0m     Embed,\n\u001b[0;32m     30\u001b[0m     Gelu,\n\u001b[0;32m     31\u001b[0m     HardSigmoid,\n\u001b[0;32m     32\u001b[0m     HardSwish,\n\u001b[0;32m     33\u001b[0m     HardSwishMobilenet,\n\u001b[0;32m     34\u001b[0m     HardTanh,\n\u001b[0;32m     35\u001b[0m     HashEmbed,\n\u001b[0;32m     36\u001b[0m     LayerNorm,\n\u001b[0;32m     37\u001b[0m     Linear,\n\u001b[0;32m     38\u001b[0m     Logistic,\n\u001b[0;32m     39\u001b[0m     Maxout,\n\u001b[0;32m     40\u001b[0m     Mish,\n\u001b[0;32m     41\u001b[0m     MultiSoftmax,\n\u001b[0;32m     42\u001b[0m     MXNetWrapper,\n\u001b[0;32m     43\u001b[0m     ParametricAttention,\n\u001b[0;32m     44\u001b[0m     ParametricAttention_v2,\n\u001b[0;32m     45\u001b[0m     PyTorchLSTM,\n\u001b[0;32m     46\u001b[0m     PyTorchRNNWrapper,\n\u001b[0;32m     47\u001b[0m     PyTorchWrapper,\n\u001b[0;32m     48\u001b[0m     PyTorchWrapper_v2,\n\u001b[0;32m     49\u001b[0m     PyTorchWrapper_v3,\n\u001b[0;32m     50\u001b[0m     Relu,\n\u001b[0;32m     51\u001b[0m     ReluK,\n\u001b[0;32m     52\u001b[0m     Sigmoid,\n\u001b[0;32m     53\u001b[0m     Softmax,\n\u001b[0;32m     54\u001b[0m     Softmax_v2,\n\u001b[0;32m     55\u001b[0m     SparseLinear,\n\u001b[0;32m     56\u001b[0m     SparseLinear_v2,\n\u001b[0;32m     57\u001b[0m     Swish,\n\u001b[0;32m     58\u001b[0m     TensorFlowWrapper,\n\u001b[0;32m     59\u001b[0m     TorchScriptWrapper_v1,\n\u001b[0;32m     60\u001b[0m     add,\n\u001b[0;32m     61\u001b[0m     array_getitem,\n\u001b[0;32m     62\u001b[0m     bidirectional,\n\u001b[0;32m     63\u001b[0m     chain,\n\u001b[0;32m     64\u001b[0m     clone,\n\u001b[0;32m     65\u001b[0m     concatenate,\n\u001b[0;32m     66\u001b[0m     expand_window,\n\u001b[0;32m     67\u001b[0m     keras_subclass,\n\u001b[0;32m     68\u001b[0m     list2array,\n\u001b[0;32m     69\u001b[0m     list2padded,\n\u001b[0;32m     70\u001b[0m     list2ragged,\n\u001b[0;32m     71\u001b[0m     map_list,\n\u001b[0;32m     72\u001b[0m     noop,\n\u001b[0;32m     73\u001b[0m     padded2list,\n\u001b[0;32m     74\u001b[0m     premap_ids,\n\u001b[0;32m     75\u001b[0m     pytorch_to_torchscript_wrapper,\n\u001b[0;32m     76\u001b[0m     ragged2list,\n\u001b[0;32m     77\u001b[0m     reduce_first,\n\u001b[0;32m     78\u001b[0m     reduce_last,\n\u001b[0;32m     79\u001b[0m     reduce_max,\n\u001b[0;32m     80\u001b[0m     reduce_mean,\n\u001b[0;32m     81\u001b[0m     reduce_sum,\n\u001b[0;32m     82\u001b[0m     remap_ids,\n\u001b[0;32m     83\u001b[0m     remap_ids_v2,\n\u001b[0;32m     84\u001b[0m     residual,\n\u001b[0;32m     85\u001b[0m     resizable,\n\u001b[0;32m     86\u001b[0m     siamese,\n\u001b[0;32m     87\u001b[0m     sigmoid_activation,\n\u001b[0;32m     88\u001b[0m     softmax_activation,\n\u001b[0;32m     89\u001b[0m     strings2arrays,\n\u001b[0;32m     90\u001b[0m     tuplify,\n\u001b[0;32m     91\u001b[0m     uniqued,\n\u001b[0;32m     92\u001b[0m     with_array,\n\u001b[0;32m     93\u001b[0m     with_array2d,\n\u001b[0;32m     94\u001b[0m     with_cpu,\n\u001b[0;32m     95\u001b[0m     with_debug,\n\u001b[0;32m     96\u001b[0m     with_flatten,\n\u001b[0;32m     97\u001b[0m     with_flatten_v2,\n\u001b[0;32m     98\u001b[0m     with_getitem,\n\u001b[0;32m     99\u001b[0m     with_list,\n\u001b[0;32m    100\u001b[0m     with_nvtx_range,\n\u001b[0;32m    101\u001b[0m     with_padded,\n\u001b[0;32m    102\u001b[0m     with_ragged,\n\u001b[0;32m    103\u001b[0m     with_reshape,\n\u001b[0;32m    104\u001b[0m     with_signpost_interval,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    107\u001b[0m     CategoricalCrossentropy,\n\u001b[0;32m    108\u001b[0m     CosineDistance,\n\u001b[0;32m    109\u001b[0m     L2Distance,\n\u001b[0;32m    110\u001b[0m     SequenceCategoricalCrossentropy,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    113\u001b[0m     Model,\n\u001b[0;32m    114\u001b[0m     change_attr_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     wrap_model_recursive,\n\u001b[0;32m    119\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\thinc\\layers\\__init__.py:39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparametricattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParametricAttention\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparametricattention_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParametricAttention_v2\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpremap_ids\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m premap_ids\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorchwrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     PyTorchRNNWrapper,\n\u001b[0;32m     42\u001b[0m     PyTorchWrapper,\n\u001b[0;32m     43\u001b[0m     PyTorchWrapper_v2,\n\u001b[0;32m     44\u001b[0m     PyTorchWrapper_v3,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged2list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged2list\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad1bdb-aea2-4ce4-bead-3b898189262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Lexeme object is an entry in the vocabulary\n",
    "# Contains the context-independent information about a word\n",
    "# Word text: lexeme.text and lexeme.orth (the hash)\n",
    "# Lexical attributes like lexeme.is_alpha\n",
    "# Not context-dependent part-of-speech tags, dependencies or entity labels\n",
    "\n",
    "# The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
    "# Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the \n",
    "# string store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a220cc5-dcaa-4de3-8394-04481de6912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ef741e-7a58-46b1-9f4e-6c1a27a8c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure - Doc and it's views Token and Span\n",
    "# The Doc class takes three arguments: the shared vocab, the words and the spaces\n",
    "# A Span is a slice of a doc consisting of one or more tokens. \n",
    "# Span three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!\n",
    "\n",
    "# Create an nlp object\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a22a70-da02-4de5-b27c-b4b6ca63454e",
   "metadata": {},
   "source": [
    "# Best Practices\n",
    "* Doc and Span are very powerful and hold references and relationships of words and sentences\n",
    "* Convert result to strings as late as possible\n",
    "* Use token attributes if available – for example, token.i for the token index\n",
    "* Don't forget to pass in the shared vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8587c1-48c2-4652-bb45-dcda1dc63788",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for index, token in enumerate(doc):\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == nlp.vocab.strings[\"PROPN\"]:\n",
    "        # Check if the next token is a verb\n",
    "        if index + 1 < len(doc) and doc[index + 1].pos_ == \"VERB\":\n",
    "            result = nlp.vocab.strings[token]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57fd3602-2387-47e1-ada1-cdb63a5210d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8382381200790405\n",
      "1.0\n",
      "0.2274085134267807\n",
      "0.5528544783592224\n"
     ]
    }
   ],
   "source": [
    "# Word vectors and semantic similarity \n",
    "# compare two objects and predict how similar they are – for example, documents, spans or single tokens.\n",
    "# Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, \n",
    "# indicating how similar they are.\n",
    "# In order to use similarity, you need a larger spaCy pipeline that has word vectors included (medium or large English pipeline ends in \"md\" or \"lg\")\n",
    "\n",
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5d51a-5ccc-4d54-bee1-0aec0dfa79c6",
   "metadata": {},
   "source": [
    "## How does spaCy predict similarity?\n",
    "* Similarity is determined using word vectors\n",
    "* Multi-dimensional meaning representations of words\n",
    "* Generated using an algorithm like Word2Vec and lots of text\n",
    "* Can be added to spaCy's pipelines\n",
    "* Default: cosine similarity, but can be adjusted\n",
    "* Doc and Span vectors default to average of token vectors\n",
    "* Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57246898-b7a1-4ca2-95a3-7deb892f1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6334    0.18981  -0.53544  -0.52658  -0.30001   0.30559  -0.49303\n",
      "  0.14636   0.012273  0.96802 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5347cf-0517-430a-8b1e-c7d90df4ef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2)) # very similar silarity score but in fact they are different "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e3c0e-7569-40f3-a5b7-912b9aa98194",
   "metadata": {},
   "source": [
    "# Combining predictions from statistical models with rule-based systems \n",
    "* Statistical models:\n",
    "  * Use cases\tapplication needs to generalize based on examples\n",
    "  * Real-world examples\tproduct names, person names, subject/object relationships\n",
    "  * spaCy features\tentity recognizer, dependency parser, part-of-speech tagger\n",
    "* Rule-based systems:\n",
    "  * dictionary with finite number of examples\n",
    "  * countries of the world, cities, drug names, dog breeds\n",
    "  * tokenizer, Matcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b6ad2-508c-4dec-aed2-846546edb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Systems\n",
    "# Initialize with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens, LOWER is case insensative\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Statistical models\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", [[{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}]])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c4d37-9cb7-4a78-89d7-dc8443061409",
   "metadata": {},
   "source": [
    "# PhraseMatcher \n",
    "* Like regular expressions or keyword search – with access to the tokens!\r",
    "* \n",
    "Takes Doc object as pattern\n",
    "* More efficient and faster than the Matcher\n",
    "* Great for matching large word listssts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303d9b3-9e81-4f63-969d-454f890c3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\") #instead of a dictionary, pass in a document \n",
    "matcher.add(\"DOG\", [pattern])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a9708-87d7-4bc8-921f-9339b2f5d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match with json file with list of countries\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e73cd20-6030-41ce-a640-54040cb6035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lexeme in nlp.vocab:\n",
    "    print(lexeme[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acaacec-9af1-4e3c-ae4c-ca681e9b79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each country name matched, the root head token and the country name are printed. This helps understand the syntactic relationship in the sentence.\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "# Load language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Initialize SpaCy tool for matching pharases\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "# nlp.pipe process COUNTRIES list as tokens\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches, matcher applied to doc\n",
    "# start and end: Indices of the matched tokens in doc\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    # This token is the syntactic \"head\" of the span (e.g., the main verb or noun it's linked to).\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    # This provides context for how the country name is used in the sentence.\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
